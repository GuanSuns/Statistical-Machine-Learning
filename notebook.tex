
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Discriminant Linear Classifiers}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Problem}\label{problem}

You are given a training data set \(\{x_n , t_n \}\) of size N = 21.
Each input vector \(x_n\) is a point in the 2-dimensional Euclidean
space \(R^2\) . We have
\(x_1 = (0, 0), x_2 = (1, 0), x_3 = (2, 0), x_4 = (0, 1), x_5 = (1, 1), x_6 = (2, 1), x_7 = (3, 1), x_8 = (4, 1), x_9 = (5, 1), x_{10} = (100, 1), x_{11} = (0, 2), x_{12} = (1, 2), x_{13} = (2, 2), x_{14} = (3, 2), x_{15} = (4, 2), x_{16} = (5, 2), x_{17} = (100, 2), x_{18} = (3, 3), x_{19} = (4, 3), x_{20} = (5, 3), x_{21} = (100, 3)\)

There are two target classes C1 and C2. For each point \(x_n\) in the
training set, \(x_n\) belongs to C1 if its second coordinate is less
than or equal to 2, and belongs to C2 otherwise. If \(x_n \in C1\), we
have \(t_n = 1\). If \(x_n \in C2\), we have \(t_n = 0\) in the
questions regarding least squares linear discriminant and Fisher's
linear discriminant, and have \(t_n = -1\) in the question on the
perceptron algorithm

    \subsection{Load the dataset}\label{load-the-dataset}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}
         \PY{k+kn}{import} \PY{n+nn}{random}
         
         \PY{k}{def} \PY{n+nf}{x2lable}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{is\PYZus{}for\PYZus{}perceptron}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}will return a list of (x, t)\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{t\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n}{x}\PY{p}{:}
                 \PY{n}{t} \PY{o}{=} \PY{l+m+mi}{1}
                 \PY{k}{if} \PY{n}{item}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{2}\PY{p}{:}
                     \PY{k}{if} \PY{n}{is\PYZus{}for\PYZus{}perceptron}\PY{p}{:}
                         \PY{n}{t} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
                     \PY{k}{else}\PY{p}{:}
                         \PY{n}{t} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{n}{t\PYZus{}data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{t}\PY{p}{)}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{t\PYZus{}data}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{n}{x\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{bias} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{n\PYZus{}data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{bias}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
             
             \PY{n}{c1\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{c2\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             
             \PY{n}{labels\PYZus{}square\PYZus{}fisher} \PY{o}{=} \PY{n}{x2lable}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{k+kc}{False}\PY{p}{)}
             \PY{n}{labels\PYZus{}perceptron} \PY{o}{=} \PY{n}{x2lable}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{n}{labels\PYZus{}perceptron}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                     \PY{n}{c1\PYZus{}data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
                 \PY{k}{else}\PY{p}{:}
                     \PY{n}{c2\PYZus{}data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
                     
             
             \PY{k}{return} \PY{p}{[}\PY{p}{[}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{,} \PY{n}{labels\PYZus{}square\PYZus{}fisher}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{,} \PY{n}{labels\PYZus{}perceptron}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{c1\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{c2\PYZus{}data}\PY{p}{)}\PY{p}{]}
         
         \PY{n}{data} \PY{o}{=} \PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
         \PY{n}{least\PYZus{}squares\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{fisher\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{perceptron\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{c1\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
         \PY{n}{c2\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of traning data: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of traning labels: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of c1 data: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c1\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of c2 data: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c2\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Shape of traning data:  (21, 3)
Shape of traning labels:  (21,)
Shape of c1 data:  (17, 2)
Shape of c2 data:  (4, 2)

    \end{Verbatim}

    \subsection{Least-Square Linear
Classifier}\label{least-square-linear-classifier}

Compute the least-square linear classifier based on the training data.
You need to write out (a) the error function, (b) the computed
parameters \((w_0 , w_1 , w_2)\), and (c) plot the classification
together with the training data

    \subsubsection{Calcualte the weights}\label{calcualte-the-weights}

In matrix form: \(w = (X^T X)^{-1} X^T Y\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{def} \PY{n+nf}{w\PYZus{}calc\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} generate the T matrix}
            \PY{n}{T} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{k}{if} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                    \PY{n}{T}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{T}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
            \PY{c+c1}{\PYZsh{} calculate the weights}
            \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{)}\PY{p}{)}
            \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{,} \PY{n}{T}\PY{p}{)}
            
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of T matrix: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{T}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of x\PYZus{}train\PYZus{}with\PYZus{}bias:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of y\PYZus{}train:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of weights:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{weights}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{weights}
        
        \PY{k}{def} \PY{n+nf}{solve\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{least\PYZus{}squares\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{least\PYZus{}squares\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         
            \PY{n}{weights} \PY{o}{=} \PY{n}{w\PYZus{}calc\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Learned weights:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} test the accuracy}
            \PY{n}{prediction} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{weights}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{o}{.}\PY{n}{T}\PY{p}{)}
            \PY{n}{n\PYZus{}test} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
            \PY{n}{n\PYZus{}correct} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{n\PYZus{}test}\PY{p}{)}\PY{p}{:}
                \PY{n}{t} \PY{o}{=} \PY{l+m+mi}{1}
                \PY{k}{if} \PY{n}{prediction}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZlt{}} \PY{n}{prediction}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}
                    \PY{n}{t} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{k}{if} \PY{n}{t} \PY{o}{==} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}
                    \PY{n}{n\PYZus{}correct} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction accuracy on training set: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n+nb}{float}\PY{p}{(}\PY{n}{n\PYZus{}correct}\PY{p}{)}\PY{o}{/}\PY{n}{n\PYZus{}test}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
            \PY{k}{return} \PY{n}{weights}
        
        \PY{n}{w\PYZus{}least\PYZus{}square} \PY{o}{=} \PY{n}{solve\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Shape of T matrix:  (21, 2)
Shape of x\_train\_with\_bias: (21, 3)
Shape of y\_train: (21,)
Shape of weights: (3, 2)
Learned weights: [[ 1.27964245e+00 -2.79642450e-01]
 [-2.05868000e-04  2.05868000e-04]
 [-2.97014152e-01  2.97014152e-01]]
Prediction accuracy on training set: 100.00\%

    \end{Verbatim}

    \paragraph{We can also solve it using gradient
descent}\label{we-can-also-solve-it-using-gradient-descent}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{def} \PY{n+nf}{loss\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
            \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
            \PY{n}{delta} \PY{o}{=} \PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{w}\PY{p}{)}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{delta}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{n\PYZus{}data}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{w\PYZus{}calc\PYZus{}least\PYZus{}square\PYZus{}gradient}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{epsilon} \PY{o}{=} \PY{l+m+mf}{0.0000001}\PY{p}{,} \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.001}\PY{p}{)}\PY{p}{:}
            \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{p}{)}\PY{p}{)}
            
            \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{old\PYZus{}loss\PYZus{}value} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{loss\PYZus{}value} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{while} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{old\PYZus{}loss\PYZus{}value} \PY{o}{\PYZhy{}} \PY{n}{loss\PYZus{}value}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{epsilon}\PY{p}{:}
                
                \PY{n}{old\PYZus{}loss\PYZus{}value} \PY{o}{=} \PY{n}{loss\PYZus{}value}
                \PY{n}{i}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
                \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{1000000}\PY{p}{:}
                    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fail to converge in }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ iteration}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{)}
                    \PY{k}{break}
                
                \PY{n}{delta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{,} \PY{n}{weights}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}train}
                \PY{n}{partial\PYZus{}derivative} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{1.0}\PY{o}{/}\PY{n}{n\PYZus{}data}\PY{p}{)}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{delta}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} update weights}
                \PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{\PYZhy{}} \PY{n}{partial\PYZus{}derivative}\PY{o}{*}\PY{n}{learning\PYZus{}rate}
                
                \PY{n}{loss\PYZus{}value} \PY{o}{=} \PY{n}{loss\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                
            \PY{k}{return} \PY{n}{weights}
        
        \PY{k}{def} \PY{n+nf}{solve\PYZus{}least\PYZus{}square\PYZus{}gradient\PYZus{}descent}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{least\PYZus{}squares\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{y\PYZus{}train\PYZus{}c1} \PY{o}{=} \PY{n}{least\PYZus{}squares\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} generate y\PYZus{}train\PYZus{}c1}
            \PY{n}{y\PYZus{}train\PYZus{}c2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{y\PYZus{}train\PYZus{}c1}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}train\PYZus{}c1}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{y\PYZus{}train\PYZus{}c2}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{y\PYZus{}train\PYZus{}c1}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
            
            \PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}c1} \PY{o}{=} \PY{n}{w\PYZus{}calc\PYZus{}least\PYZus{}square\PYZus{}gradient}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}c1}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Learned weights of classifier 1:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}c1}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final loss: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{loss\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}c1}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}c1}\PY{p}{)}\PY{p}{)}
            
            \PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}c2} \PY{o}{=} \PY{n}{w\PYZus{}calc\PYZus{}least\PYZus{}square\PYZus{}gradient}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}c2}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Learned weights of classifier 2:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}c2}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final loss: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{loss\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}c2}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}c2}\PY{p}{)}\PY{p}{)}
            
            \PY{k}{return} \PY{p}{(}\PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}c1}\PY{p}{,} \PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}c2}\PY{p}{)}
        
        \PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}gradient\PYZus{}descent} \PY{o}{=} \PY{n}{solve\PYZus{}least\PYZus{}square\PYZus{}gradient\PYZus{}descent}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Learned weights of classifier 1: [ 1.23870348e+00 -1.94278070e-04 -2.76787444e-01]
Final loss:  0.03668881434713212
Learned weights of classifier 2: [-2.38707891e-01  1.94279319e-04  2.76789623e-01]
Final loss:  0.036688765151482235

    \end{Verbatim}

    \subsubsection{Visualize the Results
(Least-Square)}\label{visualize-the-results-least-square}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{c+c1}{\PYZsh{} visualize data points}
        \PY{k}{def} \PY{n+nf}{visualize\PYZus{}data}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{,} \PY{n}{y\PYZus{}data}\PY{p}{)}\PY{p}{:}
            \PY{n}{c1\PYZus{}data\PYZus{}index} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{c2\PYZus{}data\PYZus{}index} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                \PY{k}{if} \PY{n}{y\PYZus{}data}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                    \PY{n}{c1\PYZus{}data\PYZus{}index}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{c2\PYZus{}data\PYZus{}index}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} use red dot to represent C1}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x\PYZus{}data}\PY{p}{[}\PY{n}{c1\PYZus{}data\PYZus{}index}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{c1\PYZus{}data\PYZus{}index}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{x\PYZus{}data}\PY{p}{[}\PY{n}{c1\PYZus{}data\PYZus{}index}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{c1\PYZus{}data\PYZus{}index}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rx}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} use blue dot to represent C2}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x\PYZus{}data}\PY{p}{[}\PY{n}{c2\PYZus{}data\PYZus{}index}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{c2\PYZus{}data\PYZus{}index}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{x\PYZus{}data}\PY{p}{[}\PY{n}{c2\PYZus{}data\PYZus{}index}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{c2\PYZus{}data\PYZus{}index}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bx}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
            \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{visualize\PYZus{}decision\PYZus{}boundary\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{weights1}\PY{p}{,} \PY{n}{weights2}\PY{p}{,} \PY{n}{x0\PYZus{}min}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{x0\PYZus{}max}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{decision boundary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
            \PY{n}{w10} \PY{o}{=} \PY{n}{weights1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{w11} \PY{o}{=} \PY{n}{weights1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{w12} \PY{o}{=} \PY{n}{weights1}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
            
            \PY{n}{w20} \PY{o}{=} \PY{n}{weights2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{w21} \PY{o}{=} \PY{n}{weights2}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{w22} \PY{o}{=} \PY{n}{weights2}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
            
            \PY{c+c1}{\PYZsh{} the decision boundary is where w10 + w11*x0 + w12*x1 = w20 + w21*x0 + w22*x1}
            \PY{c+c1}{\PYZsh{} which satisfies x1 = (w21 \PYZhy{} w11)/(w12 \PYZhy{} w22)*x0 + (w20 \PYZhy{} w10)/(w12 \PYZhy{} w22)}
            
            \PY{n}{x10} \PY{o}{=} \PY{p}{(}\PY{n}{w21} \PY{o}{\PYZhy{}} \PY{n}{w11}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{w12} \PY{o}{\PYZhy{}} \PY{n}{w22}\PY{p}{)}\PY{o}{*}\PY{n}{x0\PYZus{}min} \PY{o}{+} \PY{p}{(}\PY{n}{w20} \PY{o}{\PYZhy{}} \PY{n}{w10}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{w12} \PY{o}{\PYZhy{}} \PY{n}{w22}\PY{p}{)}
            \PY{n}{x11} \PY{o}{=} \PY{p}{(}\PY{n}{w21} \PY{o}{\PYZhy{}} \PY{n}{w11}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{w12} \PY{o}{\PYZhy{}} \PY{n}{w22}\PY{p}{)}\PY{o}{*}\PY{n}{x0\PYZus{}max} \PY{o}{+} \PY{p}{(}\PY{n}{w20} \PY{o}{\PYZhy{}} \PY{n}{w10}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{w12} \PY{o}{\PYZhy{}} \PY{n}{w22}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x0\PYZus{}min}\PY{p}{,} \PY{n}{x0\PYZus{}max}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{x10}\PY{p}{,} \PY{n}{x11}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{label}\PY{p}{)} 
            \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{bbox\PYZus{}to\PYZus{}anchor}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{1.05}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{borderaxespad}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{)}
            
        \PY{k}{def} \PY{n+nf}{visualize\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{visualize\PYZus{}data}\PY{p}{(}\PY{n}{least\PYZus{}squares\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{least\PYZus{}squares\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{w1} \PY{o}{=} \PY{n}{w\PYZus{}least\PYZus{}square}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{w2} \PY{o}{=} \PY{n}{w\PYZus{}least\PYZus{}square}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{visualize\PYZus{}decision\PYZus{}boundary\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{w1}\PY{p}{,} \PY{n}{w2}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Least\PYZhy{}Square Linear Classifier}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{visualize\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Results learned by gradient
descent}\label{results-learned-by-gradient-descent}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k}{def} \PY{n+nf}{visualize\PYZus{}least\PYZus{}square\PYZus{}gradient}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{visualize\PYZus{}data}\PY{p}{(}\PY{n}{least\PYZus{}squares\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{least\PYZus{}squares\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{w1} \PY{o}{=} \PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}gradient\PYZus{}descent}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{w2} \PY{o}{=} \PY{n}{w\PYZus{}least\PYZus{}square\PYZus{}gradient\PYZus{}descent}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{visualize\PYZus{}decision\PYZus{}boundary\PYZus{}least\PYZus{}square}\PY{p}{(}\PY{n}{w1}\PY{p}{,} \PY{n}{w2}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Least\PYZhy{}Square Linear Classifier}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
        \PY{n}{visualize\PYZus{}least\PYZus{}square\PYZus{}gradient}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Fisher's Linear
Discriminate}\label{fishers-linear-discriminate}

Compute the linear classifier based on the training data using Fisher's
linear discriminant. You need to write out (a) the error function, (b)
the computed parameters \((w_0 , w_1 , w_2)\), and (c) plot the
classification together with the training data.

    \subsubsection{Calcualte the weights}\label{calcualte-the-weights}

\begin{itemize}
\tightlist
\item
  Fisher criterion: to maximize \(J(w) = \frac{w^{T}S_B w}{w^{T}S_w w}\)
\item
  Fisher criterion is maximized when
  \((w^T S_B w)S_w w = (w^T S_w w)S_B w\)
\item
  We need to find a normalized vector w, such that
  \(w \propto S^{-1}_w (m_2 - m_1)\), where m1 and m2 are the mean
  vectors of the two classes
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{visualize\PYZus{}mapped\PYZus{}c1\PYZus{}c2}\PY{p}{(}\PY{n}{mapped\PYZus{}c1}\PY{p}{,} \PY{n}{mapped\PYZus{}c2}\PY{p}{,} \PY{n}{w0}\PY{p}{)}\PY{p}{:}
            \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{mapped\PYZus{}c1}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{mapped\PYZus{}c2}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{w0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{n}{w0}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{decision boundary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histogram of C1 and C2 in the mapping space}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{bbox\PYZus{}to\PYZus{}anchor}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{1.05}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{borderaxespad}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
        
        
        \PY{k}{def} \PY{n+nf}{w\PYZus{}calc\PYZus{}fisher}\PY{p}{(}\PY{n}{c1\PYZus{}train}\PY{p}{,} \PY{n}{c2\PYZus{}train}\PY{p}{)}\PY{p}{:}
            \PY{n}{dim} \PY{o}{=} \PY{n}{c1\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            
            \PY{c+c1}{\PYZsh{} calculate m1 and m2}
            \PY{n}{m1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{c1\PYZus{}train}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{m2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{c2\PYZus{}train}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m1 shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{m1}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{, m1:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{m1}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m2 shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{m2}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{, m2:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{m2}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} calculate S\PYZus{}w}
            \PY{n}{S\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{n}{dim}\PY{p}{)}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} sum up the covariance matrix of c1}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{c1\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                \PY{n}{delta} \PY{o}{=} \PY{n}{c1\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{m1}
                \PY{n}{S\PYZus{}w} \PY{o}{=} \PY{n}{S\PYZus{}w} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{delta}\PY{p}{,} \PY{n}{delta}\PY{o}{.}\PY{n}{T}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} sum up the covariance matrix of c2}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{c2\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                \PY{n}{delta} \PY{o}{=} \PY{n}{c2\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{m2}
                \PY{n}{S\PYZus{}w} \PY{o}{=} \PY{n}{S\PYZus{}w} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{delta}\PY{p}{,} \PY{n}{delta}\PY{o}{.}\PY{n}{T}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of S\PYZus{}w: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{S\PYZus{}w}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} calcaulate the weights}
            \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{S\PYZus{}w}\PY{p}{)}\PY{p}{,} \PY{n}{m1}\PY{o}{\PYZhy{}}\PY{n}{m2}\PY{p}{)}\PY{o}{.}\PY{n}{T}
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{weights}\PY{p}{)}
            \PY{n}{weights} \PY{o}{=} \PY{p}{(}\PY{n}{weights} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{weights}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{p}{)}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of weights: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{weights}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} compute the w0 by computing the mean vector of the mapped m1 and mapped m2}
            \PY{n}{m1\PYZus{}map} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{m1}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
            \PY{n}{m2\PYZus{}map} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{m2}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
            \PY{n}{c1\PYZus{}map} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{c1\PYZus{}train}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
            \PY{n}{c2\PYZus{}map} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{c2\PYZus{}train}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
            
            \PY{n}{c1\PYZus{}var} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{c1\PYZus{}map}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Varince of C1 in the mapping space: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c1\PYZus{}var}\PY{p}{)}
            \PY{n}{c2\PYZus{}var} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{c2\PYZus{}map}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Varince of C2 in the mapping space: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c2\PYZus{}var}\PY{p}{)}
            \PY{n}{w0} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{m2\PYZus{}map} \PY{o}{+} \PY{p}{(}\PY{n}{m1\PYZus{}map}\PY{o}{\PYZhy{}}\PY{n}{m2\PYZus{}map}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} visualize the mapping space}
            \PY{n}{visualize\PYZus{}mapped\PYZus{}c1\PYZus{}c2}\PY{p}{(}\PY{n}{c1\PYZus{}map}\PY{p}{,} \PY{n}{c2\PYZus{}map}\PY{p}{,} \PY{n}{w0}\PY{p}{)}
            
            \PY{k}{return} \PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{w0}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{solve\PYZus{}fisher}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{c1\PYZus{}train} \PY{o}{=} \PY{n}{c1\PYZus{}data}
            \PY{n}{c2\PYZus{}train} \PY{o}{=} \PY{n}{c2\PYZus{}data}
         
            \PY{n}{weights}\PY{p}{,} \PY{n}{w0} \PY{o}{=} \PY{n}{w\PYZus{}calc\PYZus{}fisher}\PY{p}{(}\PY{n}{c1\PYZus{}train}\PY{p}{,} \PY{n}{c2\PYZus{}train}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Learned weights: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{weights}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{, learned w0: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{w0}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} test the accuracy}
            \PY{n}{n\PYZus{}test} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{c1\PYZus{}train}\PY{p}{)} \PY{o}{+} \PY{n+nb}{len}\PY{p}{(}\PY{n}{c2\PYZus{}train}\PY{p}{)}
            \PY{n}{n\PYZus{}correct} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{prediction} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{c1\PYZus{}train}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{prediction}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{k}{if} \PY{n}{prediction}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{w0}\PY{p}{:}
                    \PY{n}{n\PYZus{}correct} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            
            \PY{n}{prediction} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{c2\PYZus{}train}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{prediction}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{k}{if} \PY{n}{prediction}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZlt{}} \PY{o}{\PYZhy{}}\PY{n}{w0}\PY{p}{:}
                    \PY{n}{n\PYZus{}correct} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                    
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction accuracy on training set: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n+nb}{float}\PY{p}{(}\PY{n}{n\PYZus{}correct}\PY{p}{)}\PY{o}{/}\PY{n}{n\PYZus{}test}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{weights}\PY{p}{,} \PY{n}{w0}
        
        \PY{n}{fisher\PYZus{}weights}\PY{p}{,} \PY{n}{fisher\PYZus{}w0} \PY{o}{=} \PY{n}{solve\PYZus{}fisher}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
m1 shape:  (2, 1) , m1:
 [[13.70588235]
 [ 1.23529412]]
m2 shape:  (2, 1) , m2:
 [[28.]
 [ 3.]]
Shape of S\_w:  (2, 2)
[[-1.34436264e-04 -1.93956675e-01]]
Shape of weights:  (2,)
Varince of C1 in the mapping space:  0.5380123251866185
Varince of C2 in the mapping space:  0.0008304100254139823
Learned weights:  [-6.93125058e-04 -9.99999760e-01] , learned w0:  [2.13210025]
Prediction accuracy on training set: 100.00\%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Visualize the Results
(Fisher)}\label{visualize-the-results-fisher}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k}{def} \PY{n+nf}{visualize\PYZus{}fisher\PYZus{}boundry}\PY{p}{(}\PY{n}{fisher\PYZus{}w}\PY{p}{,} \PY{n}{x0\PYZus{}min}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{x0\PYZus{}max}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{projection line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
            \PY{n}{x10} \PY{o}{=} \PY{p}{(}\PY{n}{fisher\PYZus{}w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{/}\PY{n}{fisher\PYZus{}w}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{n}{x0\PYZus{}min}
            \PY{n}{x11} \PY{o}{=} \PY{p}{(}\PY{n}{fisher\PYZus{}w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{/}\PY{n}{fisher\PYZus{}w}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{n}{x0\PYZus{}max}
            
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x0\PYZus{}min}\PY{p}{,} \PY{n}{x0\PYZus{}max}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{x10}\PY{p}{,} \PY{n}{x11}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{label}\PY{p}{)} 
            \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{bbox\PYZus{}to\PYZus{}anchor}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{1.05}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{borderaxespad}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
            
        \PY{k}{def} \PY{n+nf}{visualize\PYZus{}fisher}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{visualize\PYZus{}data}\PY{p}{(}\PY{n}{fisher\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{fisher\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{visualize\PYZus{}fisher\PYZus{}boundry}\PY{p}{(}\PY{n}{fisher\PYZus{}weights}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} visualize m1 and m2}
            \PY{n}{dim} \PY{o}{=} \PY{n}{c1\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{m1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{c1\PYZus{}data}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{m1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{m1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{bbox\PYZus{}to\PYZus{}anchor}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{1.05}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{borderaxespad}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{)}
            
            \PY{n}{m2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{c2\PYZus{}data}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{m2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{m2}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{co}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{bbox\PYZus{}to\PYZus{}anchor}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{1.05}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{borderaxespad}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{)}
            
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fisher}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{s Linear Classifier}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{visualize\PYZus{}fisher}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Perceptron Algorithm}\label{perceptron-algorithm}

Compute the linear classifier based on the training data using the
perceptron algorithm, starting with the initial parameter
(\(w_0 , w_1 , w_2\)) = (1.5, 0, 0). For each iteration, you need to
specify (a) the iteration number, (b) the current parameters, (c) the
misclassified input x\_n used in that particular iteration of stochastic
gradient descent, and (d) the updating vector. When the algorithm
converges, plot the classification together with the training data.

    \subsubsection{Calcualte the weights}\label{calcualte-the-weights}

\begin{itemize}
\tightlist
\item
  Minimize the error function (Perceptron Criterion):
  \(E_P(w) = \sum_{n \in M}{w^T \phi_n t_n}\)
\item
  x in C1 if \(w^T\phi(x) > 0\), and in C2 otherwise
\item
  Use stochastic gradient descent algorithm:
  \(w^{(t+1)}=w^{(t)} - \eta\nabla E_p(w) = w^{(t)} + \eta \phi_n t_n\)
\item
  If the pattern is correctly classified, then the weight vector remains
  unchanged, whereas if it is incorrectly classified, then for class C1
  we add the vector \(\phi(x_n)\) onto the current estimate of weight
  vector w while for class C2 we subtract the vector \(\phi(x_n)\) from
  w.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k}{def} \PY{n+nf}{w\PYZus{}calc\PYZus{}perceptron}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
             \PY{n}{dim} \PY{o}{=} \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
             \PY{n}{n\PYZus{}data} \PY{o}{=} \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{1.5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{is\PYZus{}converged} \PY{o}{=} \PY{k+kc}{False}
             \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{k}{while} \PY{n}{i} \PY{o}{\PYZlt{}} \PY{l+m+mi}{1000000} \PY{o+ow}{and} \PY{o+ow}{not} \PY{n}{is\PYZus{}converged}\PY{p}{:}
                 
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Iteration: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Weights: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
                 \PY{n}{i} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                 
                 \PY{n}{is\PYZus{}converged} \PY{o}{=} \PY{k+kc}{True}
                 \PY{n}{misclassified\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n}{misclassified\PYZus{}label} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 
                 \PY{c+c1}{\PYZsh{} test if converged}
                 \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{n\PYZus{}data}\PY{p}{)}\PY{p}{:}
                     \PY{n}{input\PYZus{}data} \PY{o}{=} \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{[}\PY{n}{j}\PY{p}{]}
                     \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{weights}\PY{p}{)}\PY{o}{*}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{:}
                         \PY{n}{is\PYZus{}converged} \PY{o}{=} \PY{k+kc}{False}
                         \PY{n}{misclassified\PYZus{}data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{input\PYZus{}data}\PY{p}{)}
                         \PY{n}{misclassified\PYZus{}label}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}
                         
                 \PY{c+c1}{\PYZsh{} do stochastic gradient descent using a random misclassified data}
                 \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{misclassified\PYZus{}label}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n}{selected\PYZus{}index} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{misclassified\PYZus{}label}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} 
                     \PY{n}{selected\PYZus{}data} \PY{o}{=} \PY{n}{misclassified\PYZus{}data}\PY{p}{[}\PY{n}{selected\PYZus{}index}\PY{p}{]}
                     \PY{n}{selected\PYZus{}label} \PY{o}{=} \PY{n}{misclassified\PYZus{}label}\PY{p}{[}\PY{n}{selected\PYZus{}index}\PY{p}{]}
                     \PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{+} \PY{n}{learning\PYZus{}rate}\PY{o}{*}\PY{n}{selected\PYZus{}label}\PY{o}{*}\PY{n}{selected\PYZus{}data}
                     
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Learning rate: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Misclassified input: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{selected\PYZus{}data}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Updated weights: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{weights}
         
         \PY{k}{def} \PY{n+nf}{solve\PYZus{}perceptron}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias} \PY{o}{=} \PY{n}{perceptron\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{perceptron\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
             
             \PY{n}{perceptron\PYZus{}weights} \PY{o}{=} \PY{n}{w\PYZus{}calc\PYZus{}perceptron}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}with\PYZus{}bias}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Learned weights: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{perceptron\PYZus{}weights}\PY{p}{)}
             \PY{k}{return} \PY{n}{perceptron\PYZus{}weights}
             
         \PY{n}{perceptron\PYZus{}weights} \PY{o}{=} \PY{n}{solve\PYZus{}perceptron}\PY{p}{(}\PY{p}{)}
             
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Iteration:  1
Weights:  [1.5 0.  0. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 0.5 -3.  -3. ]

Iteration:  2
Weights:  [ 0.5 -3.  -3. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 1.5  0.  -1. ]

Iteration:  3
Weights:  [ 1.5  0.  -1. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [2.5 4.  1. ]

Iteration:  4
Weights:  [2.5 4.  1. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [  1.5 -96.   -2. ]

Iteration:  5
Weights:  [  1.5 -96.   -2. ]
Learning rate:  1
Misclassified input:  [1. 1. 1.]
Updated weights:  [  2.5 -95.   -1. ]

Iteration:  6
Weights:  [  2.5 -95.   -1. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [3.5 5.  1. ]

Iteration:  7
Weights:  [3.5 5.  1. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [  2.5 -95.   -2. ]

Iteration:  8
Weights:  [  2.5 -95.   -2. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [3.5 5.  0. ]

Iteration:  9
Weights:  [3.5 5.  0. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 2.5  0.  -3. ]

Iteration:  10
Weights:  [ 2.5  0.  -3. ]
Learning rate:  1
Misclassified input:  [1. 1. 1.]
Updated weights:  [ 3.5  1.  -2. ]

Iteration:  11
Weights:  [ 3.5  1.  -2. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 2.5 -2.  -5. ]

Iteration:  12
Weights:  [ 2.5 -2.  -5. ]
Learning rate:  1
Misclassified input:  [1. 1. 2.]
Updated weights:  [ 3.5 -1.  -3. ]

Iteration:  13
Weights:  [ 3.5 -1.  -3. ]
Learning rate:  1
Misclassified input:  [1. 1. 2.]
Updated weights:  [ 4.5  0.  -1. ]

Iteration:  14
Weights:  [ 4.5  0.  -1. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 3.5 -5.  -4. ]

Iteration:  15
Weights:  [ 3.5 -5.  -4. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 4.5 -1.  -3. ]

Iteration:  16
Weights:  [ 4.5 -1.  -3. ]
Learning rate:  1
Misclassified input:  [  1. 100.   1.]
Updated weights:  [ 5.5 99.  -2. ]

Iteration:  17
Weights:  [ 5.5 99.  -2. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 4.5 94.  -5. ]

Iteration:  18
Weights:  [ 4.5 94.  -5. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 3.5 -6.  -8. ]

Iteration:  19
Weights:  [ 3.5 -6.  -8. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [ 4.5 94.  -6. ]

Iteration:  20
Weights:  [ 4.5 94.  -6. ]
Learning rate:  1
Misclassified input:  [1. 0. 1.]
Updated weights:  [ 5.5 94.  -5. ]

Iteration:  21
Weights:  [ 5.5 94.  -5. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 4.5 91.  -8. ]

Iteration:  22
Weights:  [ 4.5 91.  -8. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [  3.5  88.  -11. ]

Iteration:  23
Weights:  [  3.5  88.  -11. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [  2.5  85.  -14. ]

Iteration:  24
Weights:  [  2.5  85.  -14. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [  1.5  82.  -17. ]

Iteration:  25
Weights:  [  1.5  82.  -17. ]
Learning rate:  1
Misclassified input:  [1. 0. 2.]
Updated weights:  [  2.5  82.  -15. ]

Iteration:  26
Weights:  [  2.5  82.  -15. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [  1.5 -18.  -18. ]

Iteration:  27
Weights:  [  1.5 -18.  -18. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [  2.5 -15.  -16. ]

Iteration:  28
Weights:  [  2.5 -15.  -16. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [  3.5 -11.  -15. ]

Iteration:  29
Weights:  [  3.5 -11.  -15. ]
Learning rate:  1
Misclassified input:  [1. 0. 1.]
Updated weights:  [  4.5 -11.  -14. ]

Iteration:  30
Weights:  [  4.5 -11.  -14. ]
Learning rate:  1
Misclassified input:  [1. 5. 1.]
Updated weights:  [  5.5  -6.  -13. ]

Iteration:  31
Weights:  [  5.5  -6.  -13. ]
Learning rate:  1
Misclassified input:  [1. 5. 2.]
Updated weights:  [  6.5  -1.  -11. ]

Iteration:  32
Weights:  [  6.5  -1.  -11. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [ 7.5  1.  -9. ]

Iteration:  33
Weights:  [ 7.5  1.  -9. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [  6.5 -99.  -12. ]

Iteration:  34
Weights:  [  6.5 -99.  -12. ]
Learning rate:  1
Misclassified input:  [1. 5. 1.]
Updated weights:  [  7.5 -94.  -11. ]

Iteration:  35
Weights:  [  7.5 -94.  -11. ]
Learning rate:  1
Misclassified input:  [1. 1. 0.]
Updated weights:  [  8.5 -93.  -11. ]

Iteration:  36
Weights:  [  8.5 -93.  -11. ]
Learning rate:  1
Misclassified input:  [1. 3. 1.]
Updated weights:  [  9.5 -90.  -10. ]

Iteration:  37
Weights:  [  9.5 -90.  -10. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [10.5 10.  -8. ]

Iteration:  38
Weights:  [10.5 10.  -8. ]
Learning rate:  1
Misclassified input:  [1. 0. 2.]
Updated weights:  [11.5 10.  -6. ]

Iteration:  39
Weights:  [11.5 10.  -6. ]
Learning rate:  1
Misclassified input:  [1. 0. 2.]
Updated weights:  [12.5 10.  -4. ]

Iteration:  40
Weights:  [12.5 10.  -4. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [11.5  5.  -7. ]

Iteration:  41
Weights:  [11.5  5.  -7. ]
Learning rate:  1
Misclassified input:  [1. 0. 2.]
Updated weights:  [12.5  5.  -5. ]

Iteration:  42
Weights:  [12.5  5.  -5. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [11.5  2.  -8. ]

Iteration:  43
Weights:  [11.5  2.  -8. ]
Learning rate:  1
Misclassified input:  [1. 1. 2.]
Updated weights:  [12.5  3.  -6. ]

Iteration:  44
Weights:  [12.5  3.  -6. ]
Learning rate:  1
Misclassified input:  [1. 4. 3.]
Updated weights:  [11.5 -1.  -9. ]

Iteration:  45
Weights:  [11.5 -1.  -9. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [12.5  3.  -7. ]

Iteration:  46
Weights:  [12.5  3.  -7. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 11.5   0.  -10. ]

Iteration:  47
Weights:  [ 11.5   0.  -10. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [ 12.5 100.   -8. ]

Iteration:  48
Weights:  [ 12.5 100.   -8. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 11.5  95.  -11. ]

Iteration:  49
Weights:  [ 11.5  95.  -11. ]
Learning rate:  1
Misclassified input:  [1. 0. 2.]
Updated weights:  [12.5 95.  -9. ]

Iteration:  50
Weights:  [12.5 95.  -9. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 11.5  -5.  -12. ]

Iteration:  51
Weights:  [ 11.5  -5.  -12. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 12.5  -1.  -11. ]

Iteration:  52
Weights:  [ 12.5  -1.  -11. ]
Learning rate:  1
Misclassified input:  [1. 3. 1.]
Updated weights:  [ 13.5   2.  -10. ]

Iteration:  53
Weights:  [ 13.5   2.  -10. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [14.5  5.  -8. ]

Iteration:  54
Weights:  [14.5  5.  -8. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 13.5   0.  -11. ]

Iteration:  55
Weights:  [ 13.5   0.  -11. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [14.5  2.  -9. ]

Iteration:  56
Weights:  [14.5  2.  -9. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 13.5 -98.  -12. ]

Iteration:  57
Weights:  [ 13.5 -98.  -12. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [ 14.5 -96.  -10. ]

Iteration:  58
Weights:  [ 14.5 -96.  -10. ]
Learning rate:  1
Misclassified input:  [1. 1. 0.]
Updated weights:  [ 15.5 -95.  -10. ]

Iteration:  59
Weights:  [ 15.5 -95.  -10. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [ 16.5 -93.   -8. ]

Iteration:  60
Weights:  [ 16.5 -93.   -8. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 17.5 -89.   -6. ]

Iteration:  61
Weights:  [ 17.5 -89.   -6. ]
Learning rate:  1
Misclassified input:  [1. 1. 0.]
Updated weights:  [ 18.5 -88.   -6. ]

Iteration:  62
Weights:  [ 18.5 -88.   -6. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 19.5 -84.   -4. ]

Iteration:  63
Weights:  [ 19.5 -84.   -4. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 20.5 -80.   -3. ]

Iteration:  64
Weights:  [ 20.5 -80.   -3. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 21.5 -77.   -1. ]

Iteration:  65
Weights:  [ 21.5 -77.   -1. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 22.5 -74.    1. ]

Iteration:  66
Weights:  [ 22.5 -74.    1. ]
Learning rate:  1
Misclassified input:  [1. 3. 1.]
Updated weights:  [ 23.5 -71.    2. ]

Iteration:  67
Weights:  [ 23.5 -71.    2. ]
Learning rate:  1
Misclassified input:  [1. 1. 1.]
Updated weights:  [ 24.5 -70.    3. ]

Iteration:  68
Weights:  [ 24.5 -70.    3. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [ 25.5 -68.    5. ]

Iteration:  69
Weights:  [ 25.5 -68.    5. ]
Learning rate:  1
Misclassified input:  [1. 2. 1.]
Updated weights:  [ 26.5 -66.    6. ]

Iteration:  70
Weights:  [ 26.5 -66.    6. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [ 27.5 -64.    8. ]

Iteration:  71
Weights:  [ 27.5 -64.    8. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 28.5 -60.    9. ]

Iteration:  72
Weights:  [ 28.5 -60.    9. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 29.5 -57.   11. ]

Iteration:  73
Weights:  [ 29.5 -57.   11. ]
Learning rate:  1
Misclassified input:  [1. 1. 1.]
Updated weights:  [ 30.5 -56.   12. ]

Iteration:  74
Weights:  [ 30.5 -56.   12. ]
Learning rate:  1
Misclassified input:  [1. 2. 1.]
Updated weights:  [ 31.5 -54.   13. ]

Iteration:  75
Weights:  [ 31.5 -54.   13. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 32.5 -50.   15. ]

Iteration:  76
Weights:  [ 32.5 -50.   15. ]
Learning rate:  1
Misclassified input:  [1. 3. 1.]
Updated weights:  [ 33.5 -47.   16. ]

Iteration:  77
Weights:  [ 33.5 -47.   16. ]
Learning rate:  1
Misclassified input:  [1. 2. 0.]
Updated weights:  [ 34.5 -45.   16. ]

Iteration:  78
Weights:  [ 34.5 -45.   16. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 35.5 -42.   18. ]

Iteration:  79
Weights:  [ 35.5 -42.   18. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [ 36.5 -40.   20. ]

Iteration:  80
Weights:  [ 36.5 -40.   20. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 37.5 -36.   21. ]

Iteration:  81
Weights:  [ 37.5 -36.   21. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 38.5 -32.   22. ]

Iteration:  82
Weights:  [ 38.5 -32.   22. ]
Learning rate:  1
Misclassified input:  [1. 5. 2.]
Updated weights:  [ 39.5 -27.   24. ]

Iteration:  83
Weights:  [ 39.5 -27.   24. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 40.5 -23.   26. ]

Iteration:  84
Weights:  [ 40.5 -23.   26. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [41.5 77.  28. ]

Iteration:  85
Weights:  [41.5 77.  28. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 40.5 -23.   25. ]

Iteration:  86
Weights:  [ 40.5 -23.   25. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 39.5 -26.   22. ]

Iteration:  87
Weights:  [ 39.5 -26.   22. ]
Learning rate:  1
Misclassified input:  [  1. 100.   1.]
Updated weights:  [40.5 74.  23. ]

Iteration:  88
Weights:  [40.5 74.  23. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [39.5 71.  20. ]

Iteration:  89
Weights:  [39.5 71.  20. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 38.5 -29.   17. ]

Iteration:  90
Weights:  [ 38.5 -29.   17. ]
Learning rate:  1
Misclassified input:  [1. 2. 1.]
Updated weights:  [ 39.5 -27.   18. ]

Iteration:  91
Weights:  [ 39.5 -27.   18. ]
Learning rate:  1
Misclassified input:  [1. 5. 2.]
Updated weights:  [ 40.5 -22.   20. ]

Iteration:  92
Weights:  [ 40.5 -22.   20. ]
Learning rate:  1
Misclassified input:  [1. 2. 0.]
Updated weights:  [ 41.5 -20.   20. ]

Iteration:  93
Weights:  [ 41.5 -20.   20. ]
Learning rate:  1
Misclassified input:  [1. 5. 1.]
Updated weights:  [ 42.5 -15.   21. ]

Iteration:  94
Weights:  [ 42.5 -15.   21. ]
Learning rate:  1
Misclassified input:  [1. 4. 3.]
Updated weights:  [ 41.5 -19.   18. ]

Iteration:  95
Weights:  [ 41.5 -19.   18. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [42.5 81.  20. ]

Iteration:  96
Weights:  [42.5 81.  20. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [41.5 78.  17. ]

Iteration:  97
Weights:  [41.5 78.  17. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 40.5 -22.   14. ]

Iteration:  98
Weights:  [ 40.5 -22.   14. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 41.5 -18.   15. ]

Iteration:  99
Weights:  [ 41.5 -18.   15. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 40.5 -21.   12. ]

Iteration:  100
Weights:  [ 40.5 -21.   12. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 39.5 -24.    9. ]

Iteration:  101
Weights:  [ 39.5 -24.    9. ]
Learning rate:  1
Misclassified input:  [1. 5. 1.]
Updated weights:  [ 40.5 -19.   10. ]

Iteration:  102
Weights:  [ 40.5 -19.   10. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 41.5 -15.   12. ]

Iteration:  103
Weights:  [ 41.5 -15.   12. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 40.5 -20.    9. ]

Iteration:  104
Weights:  [ 40.5 -20.    9. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 41.5 -16.   11. ]

Iteration:  105
Weights:  [ 41.5 -16.   11. ]
Learning rate:  1
Misclassified input:  [1. 4. 3.]
Updated weights:  [ 40.5 -20.    8. ]

Iteration:  106
Weights:  [ 40.5 -20.    8. ]
Learning rate:  1
Misclassified input:  [  1. 100.   1.]
Updated weights:  [41.5 80.   9. ]

Iteration:  107
Weights:  [41.5 80.   9. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 40.5 -20.    6. ]

Iteration:  108
Weights:  [ 40.5 -20.    6. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [41.5 80.   8. ]

Iteration:  109
Weights:  [41.5 80.   8. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [40.5 77.   5. ]

Iteration:  110
Weights:  [40.5 77.   5. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [39.5 74.   2. ]

Iteration:  111
Weights:  [39.5 74.   2. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [38.5 69.  -1. ]

Iteration:  112
Weights:  [38.5 69.  -1. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 37.5 -31.   -4. ]

Iteration:  113
Weights:  [ 37.5 -31.   -4. ]
Learning rate:  1
Misclassified input:  [1. 2. 0.]
Updated weights:  [ 38.5 -29.   -4. ]

Iteration:  114
Weights:  [ 38.5 -29.   -4. ]
Learning rate:  1
Misclassified input:  [1. 5. 2.]
Updated weights:  [ 39.5 -24.   -2. ]

Iteration:  115
Weights:  [ 39.5 -24.   -2. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 40.5 -20.    0. ]

Iteration:  116
Weights:  [ 40.5 -20.    0. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 41.5 -16.    2. ]

Iteration:  117
Weights:  [ 41.5 -16.    2. ]
Learning rate:  1
Misclassified input:  [  1. 100.   1.]
Updated weights:  [42.5 84.   3. ]

Iteration:  118
Weights:  [42.5 84.   3. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [41.5 79.   0. ]

Iteration:  119
Weights:  [41.5 79.   0. ]
Learning rate:  1
Misclassified input:  [1. 4. 3.]
Updated weights:  [40.5 75.  -3. ]

Iteration:  120
Weights:  [40.5 75.  -3. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 39.5 -25.   -6. ]

Iteration:  121
Weights:  [ 39.5 -25.   -6. ]
Learning rate:  1
Misclassified input:  [1. 2. 1.]
Updated weights:  [ 40.5 -23.   -5. ]

Iteration:  122
Weights:  [ 40.5 -23.   -5. ]
Learning rate:  1
Misclassified input:  [  1. 100.   1.]
Updated weights:  [41.5 77.  -4. ]

Iteration:  123
Weights:  [41.5 77.  -4. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [40.5 72.  -7. ]

Iteration:  124
Weights:  [40.5 72.  -7. ]
Learning rate:  1
Misclassified input:  [1. 4. 3.]
Updated weights:  [ 39.5  68.  -10. ]

Iteration:  125
Weights:  [ 39.5  68.  -10. ]
Learning rate:  1
Misclassified input:  [1. 4. 3.]
Updated weights:  [ 38.5  64.  -13. ]

Iteration:  126
Weights:  [ 38.5  64.  -13. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 37.5  61.  -16. ]

Iteration:  127
Weights:  [ 37.5  61.  -16. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 36.5 -39.  -19. ]

Iteration:  128
Weights:  [ 36.5 -39.  -19. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 37.5 -35.  -17. ]

Iteration:  129
Weights:  [ 37.5 -35.  -17. ]
Learning rate:  1
Misclassified input:  [1. 2. 0.]
Updated weights:  [ 38.5 -33.  -17. ]

Iteration:  130
Weights:  [ 38.5 -33.  -17. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 39.5 -30.  -15. ]

Iteration:  131
Weights:  [ 39.5 -30.  -15. ]
Learning rate:  1
Misclassified input:  [1. 2. 1.]
Updated weights:  [ 40.5 -28.  -14. ]

Iteration:  132
Weights:  [ 40.5 -28.  -14. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 41.5 -24.  -13. ]

Iteration:  133
Weights:  [ 41.5 -24.  -13. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 42.5 -20.  -11. ]

Iteration:  134
Weights:  [ 42.5 -20.  -11. ]
Learning rate:  1
Misclassified input:  [1. 3. 1.]
Updated weights:  [ 43.5 -17.  -10. ]

Iteration:  135
Weights:  [ 43.5 -17.  -10. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 44.5 -13.   -8. ]

Iteration:  136
Weights:  [ 44.5 -13.   -8. ]
Learning rate:  1
Misclassified input:  [1. 3. 1.]
Updated weights:  [ 45.5 -10.   -7. ]

Iteration:  137
Weights:  [ 45.5 -10.   -7. ]
Learning rate:  1
Misclassified input:  [1. 5. 1.]
Updated weights:  [46.5 -5.  -6. ]

Iteration:  138
Weights:  [46.5 -5.  -6. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 45.5 -10.   -9. ]

Iteration:  139
Weights:  [ 45.5 -10.   -9. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [46.5 -6.  -7. ]

Iteration:  140
Weights:  [46.5 -6.  -7. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 45.5  -9.  -10. ]

Iteration:  141
Weights:  [ 45.5  -9.  -10. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [46.5 91.  -8. ]

Iteration:  142
Weights:  [46.5 91.  -8. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 45.5  88.  -11. ]

Iteration:  143
Weights:  [ 45.5  88.  -11. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 44.5 -12.  -14. ]

Iteration:  144
Weights:  [ 44.5 -12.  -14. ]
Learning rate:  1
Misclassified input:  [1. 4. 1.]
Updated weights:  [ 45.5  -8.  -13. ]

Iteration:  145
Weights:  [ 45.5  -8.  -13. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 46.5  -5.  -11. ]

Iteration:  146
Weights:  [ 46.5  -5.  -11. ]
Learning rate:  1
Misclassified input:  [1. 5. 2.]
Updated weights:  [47.5  0.  -9. ]

Iteration:  147
Weights:  [47.5  0.  -9. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 46.5  -3.  -12. ]

Iteration:  148
Weights:  [ 46.5  -3.  -12. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [ 47.5  97.  -10. ]

Iteration:  149
Weights:  [ 47.5  97.  -10. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 46.5  92.  -13. ]

Iteration:  150
Weights:  [ 46.5  92.  -13. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 45.5  -8.  -16. ]

Iteration:  151
Weights:  [ 45.5  -8.  -16. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 46.5  -5.  -14. ]

Iteration:  152
Weights:  [ 46.5  -5.  -14. ]
Learning rate:  1
Misclassified input:  [  1. 100.   2.]
Updated weights:  [ 47.5  95.  -12. ]

Iteration:  153
Weights:  [ 47.5  95.  -12. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 46.5  90.  -15. ]

Iteration:  154
Weights:  [ 46.5  90.  -15. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 45.5  87.  -18. ]

Iteration:  155
Weights:  [ 45.5  87.  -18. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 44.5  82.  -21. ]

Iteration:  156
Weights:  [ 44.5  82.  -21. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 43.5 -18.  -24. ]

Iteration:  157
Weights:  [ 43.5 -18.  -24. ]
Learning rate:  1
Misclassified input:  [1. 1. 2.]
Updated weights:  [ 44.5 -17.  -22. ]

Iteration:  158
Weights:  [ 44.5 -17.  -22. ]
Learning rate:  1
Misclassified input:  [1. 5. 1.]
Updated weights:  [ 45.5 -12.  -21. ]

Iteration:  159
Weights:  [ 45.5 -12.  -21. ]
Learning rate:  1
Misclassified input:  [1. 3. 2.]
Updated weights:  [ 46.5  -9.  -19. ]

Iteration:  160
Weights:  [ 46.5  -9.  -19. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [ 47.5  -7.  -17. ]

Iteration:  161
Weights:  [ 47.5  -7.  -17. ]
Learning rate:  1
Misclassified input:  [1. 5. 1.]
Updated weights:  [ 48.5  -2.  -16. ]

Iteration:  162
Weights:  [ 48.5  -2.  -16. ]
Learning rate:  1
Misclassified input:  [  1. 100.   1.]
Updated weights:  [ 49.5  98.  -15. ]

Iteration:  163
Weights:  [ 49.5  98.  -15. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 48.5  95.  -18. ]

Iteration:  164
Weights:  [ 48.5  95.  -18. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 47.5  92.  -21. ]

Iteration:  165
Weights:  [ 47.5  92.  -21. ]
Learning rate:  1
Misclassified input:  [1. 5. 3.]
Updated weights:  [ 46.5  87.  -24. ]

Iteration:  166
Weights:  [ 46.5  87.  -24. ]
Learning rate:  1
Misclassified input:  [1. 0. 2.]
Updated weights:  [ 47.5  87.  -22. ]

Iteration:  167
Weights:  [ 47.5  87.  -22. ]
Learning rate:  1
Misclassified input:  [1. 3. 3.]
Updated weights:  [ 46.5  84.  -25. ]

Iteration:  168
Weights:  [ 46.5  84.  -25. ]
Learning rate:  1
Misclassified input:  [  1. 100.   3.]
Updated weights:  [ 45.5 -16.  -28. ]

Iteration:  169
Weights:  [ 45.5 -16.  -28. ]
Learning rate:  1
Misclassified input:  [1. 2. 2.]
Updated weights:  [ 46.5 -14.  -26. ]

Iteration:  170
Weights:  [ 46.5 -14.  -26. ]
Learning rate:  1
Misclassified input:  [1. 5. 2.]
Updated weights:  [ 47.5  -9.  -24. ]

Iteration:  171
Weights:  [ 47.5  -9.  -24. ]
Learning rate:  1
Misclassified input:  [1. 0. 2.]
Updated weights:  [ 48.5  -9.  -22. ]

Iteration:  172
Weights:  [ 48.5  -9.  -22. ]
Learning rate:  1
Misclassified input:  [1. 4. 2.]
Updated weights:  [ 49.5  -5.  -20. ]

Iteration:  173
Weights:  [ 49.5  -5.  -20. ]
Learning rate:  1
Misclassified input:  [1. 5. 2.]
Updated weights:  [ 50.5   0.  -18. ]

Iteration:  174
Weights:  [ 50.5   0.  -18. ]

Learned weights:  [ 50.5   0.  -18. ]

    \end{Verbatim}

    \subsubsection{Visualize the Results
(Perceptron)}\label{visualize-the-results-perceptron}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k}{def} \PY{n+nf}{visualize\PYZus{}perceptron\PYZus{}boundry}\PY{p}{(}\PY{n}{perceptron\PYZus{}weights}\PY{p}{,} \PY{n}{x0\PYZus{}min}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{x0\PYZus{}max}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{decision boundary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} the decision boundary is w0 + w1*x0 + w2*x1 = 0}
             \PY{c+c1}{\PYZsh{} that is x1 = (\PYZhy{}w1/w2)*x0 + (\PYZhy{}w0/w2)}
             \PY{n}{x10} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{perceptron\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{/}\PY{n}{perceptron\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{n}{x0\PYZus{}min} \PY{o}{+} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{perceptron\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{/}\PY{n}{perceptron\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
             \PY{n}{x11} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{perceptron\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{/}\PY{n}{perceptron\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{n}{x0\PYZus{}max} \PY{o}{+} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{perceptron\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{/}\PY{n}{perceptron\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x0\PYZus{}min}\PY{p}{,} \PY{n}{x0\PYZus{}max}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{x10}\PY{p}{,} \PY{n}{x11}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{label}\PY{p}{)} 
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{bbox\PYZus{}to\PYZus{}anchor}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{1.05}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{borderaxespad}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{visualize\PYZus{}perceptron}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{visualize\PYZus{}data}\PY{p}{(}\PY{n}{perceptron\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{perceptron\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{visualize\PYZus{}perceptron\PYZus{}boundry}\PY{p}{(}\PY{n}{perceptron\PYZus{}weights}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Perceptron Classifier}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
         \PY{n}{visualize\PYZus{}perceptron}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
